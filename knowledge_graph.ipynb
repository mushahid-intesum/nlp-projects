{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mushahid/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import math\n",
    "import torch\n",
    "\n",
    "import wikipedia\n",
    "from newspaper import Article, ArticleException\n",
    "from GoogleNews import GoogleNews\n",
    "\n",
    "from pyvis.network import Network\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://huggingface.co/Babelscape/rebel-large\n",
    "def extract_relations_from_model_output(text):\n",
    "    relations = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    text_replaced = text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
    "    for token in text_replaced.split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                relations.append({\n",
    "                    'head': subject.strip(),\n",
    "                    'type': relation.strip(),\n",
    "                    'tail': object_.strip()\n",
    "                })\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                relations.append({\n",
    "                    'head': subject.strip(),\n",
    "                    'type': relation.strip(),\n",
    "                    'tail': object_.strip()\n",
    "                })\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        relations.append({\n",
    "            'head': subject.strip(),\n",
    "            'type': relation.strip(),\n",
    "            'tail': object_.strip()\n",
    "        })\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeBase():\n",
    "    def __init__(self):\n",
    "        self.relations = []\n",
    "        self.entities = {}\n",
    "        self.sources = {}\n",
    "\n",
    "    def are_relations_equal(self, r1, r2):\n",
    "        return all(r1[attr] == r2[attr] for attr in ['head', 'type', 'tail'])\n",
    "    \n",
    "    def relation_exists(self, r1):\n",
    "        return any(self.are_relations_equal(r1, r2) for r2 in self.relations)\n",
    "    \n",
    "    def add_relations(self, r, article_title, article_publish_date):\n",
    "        candidate_entities = [r['head'], r['tail']]\n",
    "        entities = [self.get_wikipedia_data(ent) for ent in candidate_entities]\n",
    "\n",
    "        if any(ent is None for ent in entities):\n",
    "            return\n",
    "        \n",
    "        for e in entities:\n",
    "            self.add_entity(e)\n",
    "            \n",
    "        r['head'] = entities[0]['title']\n",
    "        r['tail'] = entities[1]['title']\n",
    "\n",
    "        article_url = list(r['meta'].keys())[0]\n",
    "        if article_url not in self.sources:\n",
    "            self.sources[article_url] = {\n",
    "                'article_title': article_title,\n",
    "                'article_publish_date': article_publish_date\n",
    "            }\n",
    "\n",
    "        if not self.relation_exists(r):\n",
    "            self.relations.append(r)\n",
    "\n",
    "        else:\n",
    "            self.merge_relations(r)\n",
    "            \n",
    "    def print(self):\n",
    "        print(\"Entities:\")\n",
    "        for e in self.entities.items():\n",
    "            print(f\"  {e}\")\n",
    "        print(\"Relations:\")\n",
    "        for r in self.relations:\n",
    "            print(f\"  {r}\")\n",
    "        print(\"Sources:\")\n",
    "        for s in self.sources.items():\n",
    "            print(f\"  {s}\")\n",
    "\n",
    "    def merge_with_kb(self, kb2):\n",
    "        for r in kb2.relations:\n",
    "            article_url = list(r['meta'].keys())[0]\n",
    "            source_data = kb2.sources(article_url)\n",
    "\n",
    "            self.add_relations(r, source_data['article_title'], source_data['article_publish_date'])\n",
    "\n",
    "    def merge_relations(self, r1):\n",
    "        r2 = [r for r in self.relations if self.are_relations_equal(r1, r)][0]\n",
    "\n",
    "        article_url = list(r1['meta'].keys())[0]\n",
    "\n",
    "        if article_url not in r2['meta']:\n",
    "            r2['meta'][article_url] = r1['meta'][article_url]\n",
    "\n",
    "        else:\n",
    "            spans_to_add = [span for span in r1['meta'][article_url]['span'] \n",
    "                            if span not in r2['meta'][article_url]['span']]\n",
    "\n",
    "            r2['meta'][article_url]['span'] += spans_to_add\n",
    "\n",
    "    def get_wikipedia_data(self, candidate_entity):\n",
    "        try:\n",
    "            page = wikipedia.page(candidate_entity)\n",
    "\n",
    "            entity_data = {\n",
    "                \"title\": page.title,\n",
    "                \"url\": page.url,\n",
    "                \"summary\": page.summary\n",
    "            }\n",
    "\n",
    "            return entity_data\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    def add_entity(self, e):\n",
    "        self.entities[e['title']] = {k:v for k,v in e.items() if k != 'title'}\n",
    "    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_small_text_to_kb(text, verbose=False):\n",
    "    kb = KnowledgeBase()\n",
    "\n",
    "    model_inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Num tokens: {len(model_inputs['input_ids'][0])}\")\n",
    "        \n",
    "    kwargs = {\n",
    "        \"max_length\": 216,\n",
    "        \"length_penalty\": 0,\n",
    "        \"num_beams\": 3,\n",
    "        \"num_return_sequences\": 3\n",
    "    }   \n",
    "\n",
    "    generated_tokens = model.generate(**model_inputs, **kwargs)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "\n",
    "    for sentence_pred in decoded_preds:\n",
    "        relations = extract_relations_from_model_output(sentence_pred)\n",
    "        for r in relations:\n",
    "            kb.add_relations(r)\n",
    "\n",
    "    return kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_text_to_kb(text, article_url, span_length=128, article_title=None,\n",
    "                    article_publish_date=None, verbose=False):\n",
    "    inputs = tokenizer([text], return_tensors='pt')\n",
    "\n",
    "    num_tokens = len(inputs['input_ids'][0])\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Input has {num_tokens} tokens\")\n",
    "\n",
    "    num_spans = math.ceil(num_tokens / span_length)\n",
    "    if verbose:\n",
    "        print(f\"Input has {num_spans} spans\")\n",
    "\n",
    "    overlap = math.ceil((num_spans * span_length - num_tokens) / max(num_spans-1, 1))\n",
    "\n",
    "    spans_boundaries = []\n",
    "    start = 0\n",
    "\n",
    "    for i in range(num_spans):\n",
    "        spans_boundaries.append([start + span_length * i, start + span_length * (i+1)])\n",
    "\n",
    "        start -= overlap\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Span boundaries are {spans_boundaries}\")\n",
    "    \n",
    "    tensor_ids = [inputs['input_ids'][0][boundary[0]:boundary[1]] for boundary in spans_boundaries]\n",
    "    tensor_masks = [inputs['attention_mask'][0][boundary[0]:boundary[1]] for boundary in spans_boundaries]\n",
    "\n",
    "    inputs = {\n",
    "        \"input_ids\": torch.stack(tensor_ids),\n",
    "        \"attention_mask\": torch.stack(tensor_masks)\n",
    "    }\n",
    "\n",
    "    num_return_sequences = 3\n",
    "    kwargs = {\n",
    "        \"max_length\": 256,\n",
    "        \"length_penalty\": 0,\n",
    "        \"num_beams\": 3,\n",
    "        \"num_return_sequences\": num_return_sequences\n",
    "    }\n",
    "\n",
    "    generated_tokens = model.generate(**inputs, **kwargs)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "\n",
    "    kb = KnowledgeBase()\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for sentence_pred in decoded_preds:\n",
    "        current_span_index = i // num_return_sequences\n",
    "        relations = extract_relations_from_model_output(sentence_pred)\n",
    "\n",
    "        for relation in relations:\n",
    "            relation['meta'] = {\n",
    "                article_url : {\n",
    "                    \"span\": [spans_boundaries[current_span_index]]\n",
    "                }\n",
    "            }\n",
    "\n",
    "            kb.add_relations(relation, article_title, article_publish_date)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "\n",
    "    return article\n",
    "\n",
    "def from_url_to_kb(url):\n",
    "    article = get_article(url)\n",
    "    config = {\n",
    "        \"article_title\": article.title,\n",
    "        \"article_publish_date\": article.publish_date\n",
    "    }\n",
    "\n",
    "    kb = from_text_to_kb(article.title, article.url, **config)\n",
    "\n",
    "    return kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_links(query, lang='en', region='US', pages=1, max_links=100000):\n",
    "    googlenews = GoogleNews(lang=lang, region=region)\n",
    "    googlenews.search(query)\n",
    "\n",
    "    all_urls = []\n",
    "\n",
    "    for page in range(pages):\n",
    "        googlenews.get(page)\n",
    "        all_urls += googlenews.get_links()\n",
    "\n",
    "    return list(set(all_urls))[:max_links]\n",
    "\n",
    "def from_urls_to_kb(urls, verbose=False):\n",
    "    kb = KnowledgeBase()\n",
    "    if verbose:\n",
    "        print(f\"{len(urls)} links to visit\")\n",
    "\n",
    "    for url in urls:\n",
    "        if verbose:\n",
    "            print(f\"Visiting {url}...\")\n",
    "\n",
    "        try:\n",
    "            kb_url = from_url_to_kb(url)\n",
    "            kb.merge_with_kb(kb_url)\n",
    "\n",
    "        except ArticleException:\n",
    "            if verbose:\n",
    "                print(f\"  Couldn't download article at url {url}\")\n",
    "    return kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has 133 tokens\n",
      "Input has 2 spans\n",
      "Span boundaries are [[0, 128], [5, 133]]\n"
     ]
    }
   ],
   "source": [
    "text = \"Napoleon Bonaparte (born Napoleone di Buonaparte; 15 August 1769 – 5 \" \\\n",
    "\"May 1821), and later known by his regnal name Napoleon I, was a French military \" \\\n",
    "\"and political leader who rose to prominence during the French Revolution and led \" \\\n",
    "\"several successful campaigns during the Revolutionary Wars. He was the de facto \" \\\n",
    "\"leader of the French Republic as First Consul from 1799 to 1804. As Napoleon I, \" \\\n",
    "\"he was Emperor of the French from 1804 until 1814 and again in 1815. Napoleon's \" \\\n",
    "\"political and cultural legacy has endured, and he has been one of the most \" \\\n",
    "\"celebrated and controversial leaders in world history.\"\n",
    "\n",
    "# kb = from_small_text_to_kb(text, verbose=True)\n",
    "\n",
    "kb = from_text_to_kb(text, verbose=True)\n",
    "kb.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_network_html(kb, filename='network.html'):\n",
    "    net = Network(directed=True, width='auto', height=\"700px\", bgcolor=\"#eeeeee\")\n",
    "\n",
    "    color_entity = '#00FF00'\n",
    "\n",
    "    for e in kb.entities:\n",
    "        net.add_node(e, shape='circle', color=color_entity)\n",
    "        \n",
    "    for r in kb.relations:\n",
    "        net.add_edge(r['head'], r['tail'], title=r['type'], label=r['type'])\n",
    "\n",
    "    net.repulsion(\n",
    "        node_distance=200,\n",
    "        central_gravity=0.2,\n",
    "        spring_length=200,\n",
    "        spring_strength=0.05,\n",
    "        damping=0.09\n",
    "    )\n",
    "    net.set_edge_smooth('dynamic')\n",
    "    net.show(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_links = get_news_links(\"Google\", pages=5, max_links=20)\n",
    "kb = from_urls_to_kb(news_links, verbose=True)\n",
    "filename = \"network_3_google.html\"\n",
    "save_network_html(kb, filename=filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
